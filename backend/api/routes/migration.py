from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from typing import List, Optional
from datetime import datetime
from bson import ObjectId
from config.database import get_database
from middleware.auth_middleware import get_current_user
from streaming.websocket_manager import websocket_manager
from services.cloud.aws_handler import AWSHandler
from services.cloud.azure_handler import AzureHandler
from services.cloud.gcp_handler import GCPHandler
from utils.encryption import decrypt_credentials
from services.metrics.performance_tracker import performance_tracker
import asyncio
import random
import os
import tempfile
import time

router = APIRouter(prefix="/api/v1/migration", tags=["migration"])

async def simulate_migration(job_id: str, user_id: str):
    collection = get_database()["migration_jobs"]
    data_collection = get_database()["data_objects"]
    creds_collection = get_database()["cloud_credentials"]
    job = collection.find_one({"_id": ObjectId(job_id)})
    if not job:
        return
    data_obj = data_collection.find_one({"_id": ObjectId(job["object_id"])})
    if not data_obj:
        return
    start_time = time.time()
    collection.update_one({"_id": ObjectId(job_id)}, {"$set": {"status": "in_progress", "progress": 0, "start_time": datetime.utcnow()}})
    await websocket_manager.send_personal({"type": "migration_update", "job_id": job_id, "progress": 0, "status": "in_progress"}, user_id)
    try:
        target_location = job["target_location"]
        
        
        await asyncio.sleep(1)
        collection.update_one({"_id": ObjectId(job_id)}, {"$set": {"progress": 25}})
        await websocket_manager.send_personal({"type": "migration_update", "job_id": job_id, "progress": 25, "status": "in_progress"}, user_id)
        
        
        if target_location in ['aws', 'azure', 'gcp']:
            
            cred = creds_collection.find_one({"user_id": user_id, "provider": target_location, "is_active": True})
            
            if cred:
                
                decrypted = decrypt_credentials(cred["credentials_encrypted"])
                
                
                await asyncio.sleep(1)
                collection.update_one({"_id": ObjectId(job_id)}, {"$set": {"progress": 50}})
                await websocket_manager.send_personal({"type": "migration_update", "job_id": job_id, "progress": 50, "status": "in_progress"}, user_id)
                
                
                with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as tmp:
                    tmp.write(f"CloudFlow File: {data_obj['name']}\n")
                    tmp.write(f"Size: {data_obj['size_bytes']} bytes\n")
                    tmp.write(f"Created: {data_obj['created_at']}\n")
                    tmp.write(f"User: {user_id}\n")
                    tmp.write(f"Original Location: {data_obj['current_location']}\n")
                    tmp.write(f"Content: This is a sample file generated by CloudFlow Intelligence Platform\n")
                    tmp_path = tmp.name
                
                
                destination_key = f"cloudflow/{user_id}/{data_obj['name']}"
                
                if target_location == 'aws':
                    import boto3
                    
                    s3_client = boto3.client('s3',
                        aws_access_key_id=decrypted['access_key_id'],
                        aws_secret_access_key=decrypted['secret_access_key'],
                        region_name=decrypted.get('region', 'us-east-1')
                    )
                    bucket_name = decrypted.get('bucket_name', 'cloudflow-data')
                    
                    
                    s3_client.upload_file(tmp_path, bucket_name, destination_key)
                    
                    
                    cloud_url = f"s3://{bucket_name}/{destination_key}"
                    data_collection.update_one(
                        {"_id": ObjectId(job["object_id"])},
                        {"$set": {"cloud_url": cloud_url, "cloud_key": destination_key}}
                    )
                
                elif target_location == 'azure':
                    from azure.storage.blob import BlobServiceClient
                    
                    connection_string = f"DefaultEndpointsProtocol=https;AccountName={decrypted['account_name']};AccountKey={decrypted['account_key']};EndpointSuffix=core.windows.net"
                    blob_service_client = BlobServiceClient.from_connection_string(connection_string)
                    container_name = decrypted.get('container_name', 'cloudflow-data')
                    
                    
                    blob_client = blob_service_client.get_blob_client(
                        container=container_name,
                        blob=destination_key
                    )
                    with open(tmp_path, 'rb') as data:
                        blob_client.upload_blob(data, overwrite=True)
                    
                    
                    cloud_url = f"https://{decrypted.get('account_name', 'storage')}.blob.core.windows.net/{container_name}/{destination_key}"
                    data_collection.update_one(
                        {"_id": ObjectId(job["object_id"])},
                        {"$set": {"cloud_url": cloud_url, "cloud_key": destination_key}}
                    )
                
                elif target_location == 'gcp':
                    from google.cloud import storage
                    import json as json_lib
                    
                    service_account_info = json_lib.loads(decrypted['service_account_json'])
                    storage_client = storage.Client.from_service_account_info(
                        service_account_info
                    )
                    bucket_name = decrypted.get('bucket_name', 'cloudflow-data')
                    
                    
                    bucket = storage_client.bucket(bucket_name)
                    blob = bucket.blob(destination_key)
                    blob.upload_from_filename(tmp_path)
                    
                    
                    cloud_url = f"gs://{bucket_name}/{destination_key}"
                    data_collection.update_one(
                        {"_id": ObjectId(job["object_id"])},
                        {"$set": {"cloud_url": cloud_url, "cloud_key": destination_key}}
                    )
                
                
                os.unlink(tmp_path)
        
        
        await asyncio.sleep(1)
        collection.update_one({"_id": ObjectId(job_id)}, {"$set": {"progress": 75}})
        await websocket_manager.send_personal({"type": "migration_update", "job_id": job_id, "progress": 75, "status": "in_progress"}, user_id)
        
        
        await asyncio.sleep(1)
        data_collection.update_one({"_id": ObjectId(job["object_id"])}, {"$set": {"current_location": job["target_location"], "current_tier": job["target_tier"], "updated_at": datetime.utcnow()}})
        collection.update_one({"_id": ObjectId(job_id)}, {"$set": {"status": "completed", "progress": 100, "end_time": datetime.utcnow()}})
        
        
        await websocket_manager.send_personal({"type": "migration_complete", "job_id": job_id, "object_id": job["object_id"], "object_name": data_obj['name']}, user_id)
        await performance_tracker.track_migration_performance(job_id, "migration", start_time, True, data_obj['size_bytes'])
        try:
            from services.alerts.email_notifier import EmailNotifier
            email_service = EmailNotifier()
            user_collection = get_database()["users"]
            user_doc = user_collection.find_one({"_id": ObjectId(user_id)})
            if user_doc:
                user_email = user_doc.get('email', 'demo@cloudflow.com')
                subject = f"âœ… Migration Completed: {data_obj['name']}"
                body = f"""Hello,

Your file migration has completed successfully!

File: {data_obj['name']}
Size: {data_obj['size_bytes']} bytes
From: {job['source_location']}
To: {job['target_location']}
Completed: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}

This is an automated notification from CloudFlow Intelligence Platform.

Best regards,
CloudFlow Team
"""
                await email_service.send_custom_email([user_email], subject, body)
                print(f"âœ… Email notification sent to {user_email} for migration {job_id}")
        except Exception as e:
            print(f"ðŸ“§ Email service demo - would send to user (SMTP not configured): {e}")
        
    except Exception as e:
        
        import traceback
        error_details = {
            "error": str(e),
            "type": type(e).__name__,
            "traceback": traceback.format_exc()
        }
        print(f"Migration failed for job {job_id}: {error_details}")
        collection.update_one({"_id": ObjectId(job_id)}, {"$set": {"status": "failed", "error": str(e), "end_time": datetime.utcnow()}})
        await websocket_manager.send_personal({"type": "migration_failed", "job_id": job_id, "error": str(e)}, user_id)

@router.post("/trigger")
async def trigger_user_migration(object_id: str, target_location: str, target_tier: Optional[str] = None, background_tasks: BackgroundTasks = None, current_user: dict = Depends(get_current_user)):
    data_collection = get_database()["data_objects"]
    migration_collection = get_database()["migration_jobs"]
    data_obj = data_collection.find_one({"_id": ObjectId(object_id), "user_id": current_user["sub"]})
    if not data_obj:
        raise HTTPException(status_code=404, detail="Data object not found")
    if data_obj["current_location"] == target_location:
        raise HTTPException(status_code=400, detail="Already at target location")
    job = {"user_id": current_user["sub"], "object_id": object_id, "object_name": data_obj["name"], "source_location": data_obj["current_location"], "source_tier": data_obj.get("current_tier", "warm"), "target_location": target_location, "target_tier": target_tier or data_obj.get("current_tier", "warm"), "size_bytes": data_obj["size_bytes"], "status": "pending", "progress": 0, "created_at": datetime.utcnow(), "metadata": {"initiated_by": current_user["email"]}}
    result = migration_collection.insert_one(job)
    job_id = str(result.inserted_id)
    background_tasks.add_task(simulate_migration, job_id, current_user["sub"])
    return {"status": "migration_initiated", "job_id": job_id, "object_id": object_id, "target": target_location}

@router.get("/")
async def list_user_migrations(status: Optional[str] = None, limit: int = 50, current_user: dict = Depends(get_current_user)):
    collection = get_database()["migration_jobs"]
    query = {"user_id": current_user["sub"]}
    if status:
        query["status"] = status
    jobs = list(collection.find(query).sort("created_at", -1).limit(limit))
    for job in jobs:
        job["_id"] = str(job["_id"])
    return jobs

@router.get("/{job_id}")
async def get_user_migration_status(job_id: str, current_user: dict = Depends(get_current_user)):
    collection = get_database()["migration_jobs"]
    job = collection.find_one({"_id": ObjectId(job_id), "user_id": current_user["sub"]})
    if not job:
        raise HTTPException(status_code=404, detail="Migration job not found")
    job["_id"] = str(job["_id"])
    return job

@router.post("/{job_id}/cancel")
async def cancel_user_migration(job_id: str, current_user: dict = Depends(get_current_user)):
    collection = get_database()["migration_jobs"]
    job = collection.find_one({"_id": ObjectId(job_id), "user_id": current_user["sub"]})
    if not job:
        raise HTTPException(status_code=404, detail="Migration job not found")
    if job["status"] not in ["pending", "in_progress"]:
        raise HTTPException(status_code=400, detail="Cannot cancel completed migration")
    collection.update_one({"_id": ObjectId(job_id)}, {"$set": {"status": "cancelled", "end_time": datetime.utcnow()}})
    return {"status": "cancelled", "job_id": job_id}
